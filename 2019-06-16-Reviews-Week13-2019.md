
# PipeLine Template  0.1

**Pay attention: Read every line carefully!!!**

- Import libs

- Load data

- Data overview

- Feature preprocessing
  
  - Numeric features
  
  - Categorical and ordinal features
  
  - Datetime and coordinates
  
  - Missing values
  
  - text and images

- EDA 
  - Get domain knowledge
  - Check if the data is intuitive
  - Understand how the data was generated

  ---

  - Explore indeividual features
  - Explore pairs and groups

  ---

  - Clean features up
  - Check for leaks!

## Import libs


```python
# numpy,pandas,os
import numpy as np
import pandas as pd
import os
```

## Loda data


```python
DATA_FOLDER = '../input'

train = pd.read_csv(os.path.join(DATA_FOLDER,'train.csv'))

# ... 
```

## Data overview


```python
train.head() 
train.T.head()
```


```python
train.describe()
```

### Check NaNs values

Number of NaNs for each object(rows)


```python
train.isnull().sum(axis=1).head(15)
```

Number of NaNs for each column


```python
train.isnull.sum(axis=0).head(15)
```

It is also useful to know if there are any NaNs in the data. You should pay attention to columns with NaNs and the number of NaNs for each row can serve as a nice feature later.

If the rows order has some structure, so the rows are not shuffled. We could use row index as another features.

## Dataset cleaning

### 1. Concatenate train and test

It is usually convenient to concatenate train and test into one dataframe and do all feature engineering using it.


```python
traintest = pd.concat([train, test], axis = 0)
```

### 2. Remove constant features

First we schould look for a constant features, such features do not provide any information and only make our dataset larger.


```python
# `dropna = False` makes nunique treat NaNs as a distinct value
feats_counts = train.nunique(dropna = False)
```


```python
feats_counts.sort_values()[:10] # Check last ten recorders
```

`feats_counts==1` is constant features. Remove them.


```python
constant_features = feats_counts.loc[feats_counts==1].index.tolist()
print (constant_features)


traintest.drop(constant_features,axis = 1,inplace=True)
```

### 3. Remove duplicated features

3.0 Firstly, fill NaNs with something we can find later if needed.


```python
traintest.fillna('NaN', inplace=True)
```

3.1 Before check duplicted we need encode each feature using LabelEncoder. Because some duplicted feature that be found after encode.


```python
train_enc =  pd.DataFrame(index = train.index)

for col in tqdm_notebook(traintest.columns):
    train_enc[col] = train[col].factorize()[0]
```

Another way to encoder


```python
train_enc[col] = train[col].map(train[col].value_counts())
```

3.2 Way One: for less data using `.duplicated`


```python
# check duplicated(row data total duplicated)

df.duplicated().any()

# delete duplicated row and leave the lastone and replace the source data

df.drop_duplicates(keep='last', inplace=True)
```

3.2 Way Two: If the resulting data frame is very very large, we cannot just transpose it and use `.duplicated`. That is why we will use a simple loop.


```python
dup_cols = {}

for i, c1 in enumerate(tqdm_notebook(train_enc.columns)):
    for c2 in train_enc.columns[i + 1:]:
        if c2 not in dup_cols and np.all(train_enc[c1] == train_enc[c2]):
            dup_cols[c2] = c1
```


```python
dup_cols
```

Save the duplicated data.


```python
import cPickle as pickle
pickle.dump(dup_cols, open('dup_cols.p', 'w'), protocol=pickle.HIGHEST_PROTOCOL)
```

Drop from traintest.


```python
traintest.drop(dup_cols.keys(), axis = 1,inplace=True)
```

## Detemine types

Let's examine the number of unique values

`dropna = False` to make sure this function computes and account for NaNs


```python
nunique = train.nunique(dropna=False)
nunique
```

and build a histogram of thost values, it's histogram of those normalized values.


```python
plt.figure(figsize=(14,6))
_ = plt.hist(nunique.astype(float)/train.shape[0], bins=100)
```

Let's take a looks at the features with a huge number of unique values:


```python
mask = (nunique.astype(float)/train.shape[0] > 0.8)
train.loc[:, mask]
```

The values are not float, they are integer, so these features are likely to be even counts. Let's look at another pack of features.


```python
mask = (nunique.astype(float)/train.shape[0] < 0.8) & (nunique.astype(float)/train.shape[0] > 0.4)
train.loc[:25, mask]
```

These look like counts too. First thing to notice is the 23th line: 99999.., -99999 values look like NaNs so we should probably built a related feature. Second: the columns are sometimes placed next to each other, so the columns are probably grouped together and we can disentangle that.

Our conclusion: there are no floating point variables, there are some counts variables, which we will treat as numeric.

And finally, let's pick one variable (in this case 'VAR_0015') from the third group of features.


```python
# Calculate how many rows are there in one column and group by each values 
train['VAR_0015'].value_counts()
```

Filter the columns and sperate columns in to categorical and numeric.


```python
cat_cols = list(train.select_dtypes(include=['object']).columns) # Categorical columns
num_cols = list(train.select_dtypes(exclude=['object']).columns) # Numeric columns
```

## Go through

### 1. Replace NaNs with something first


```python
train.replace('NaN', -999, inplace=True)
```

Let's calculate how many times one feature is greater than the other and create cross tabel out of it. 


```python
# select first 42 numeric features
feats = num_cols[:42]

# build 'mean(feat1 > feat2)' plot
gt_matrix(feats,16)
```

## Investigation


```python

```


```python

```

## EDA

### Get domain Knowledge 


```python

```


```python

```


```python

```


```python

```


```python

```


```python

```


```python

```


```python

```

## Tools


```python
def autolabel(arrayA):
    ''' label each colored square with the corresponding data value. 
    If value > 20, the text is in black, else in white.
    '''
    arrayA = np.array(arrayA)
    for i in range(arrayA.shape[0]):
        for j in range(arrayA.shape[1]):
                plt.text(j,i, "%.2f"%arrayA[i,j], ha='center', va='bottom',color='w')

def hist_it(feat):
    plt.figure(figsize=(16,4))
    feat[Y==0].hist(bins=range(int(feat.min()),int(feat.max()+2)),normed=True,alpha=0.8)
    feat[Y==1].hist(bins=range(int(feat.min()),int(feat.max()+2)),normed=True,alpha=0.5)
    plt.ylim((0,1))
    
def gt_matrix(feats,sz=16):
    a = []
    for i,c1 in enumerate(feats):
        b = [] 
        for j,c2 in enumerate(feats):
            mask = (~train[c1].isnull()) & (~train[c2].isnull())
            if i>=j:
                b.append((train.loc[mask,c1].values>=train.loc[mask,c2].values).mean())
            else:
                b.append((train.loc[mask,c1].values>train.loc[mask,c2].values).mean())

        a.append(b)

    plt.figure(figsize = (sz,sz))
    plt.imshow(a, interpolation = 'None')
    _ = plt.xticks(range(len(feats)),feats,rotation = 90)
    _ = plt.yticks(range(len(feats)),feats,rotation = 0)
    autolabel(a)
```


