# Stacking - How to win a data science competiton

## Methodology

Wolpert in 1992 introduced stacking. It involves:

1. **Splitting** the train set into two disjoint sets.
2. **Train** several base learners on the first part.
3. **Make predictions** with the base learners on the second(calidation)part.
4. Using the **predictions** from (3) **as the inputs** to train a higher level learner.

![2.png](2.png)

**Stacking example**

```python
from sklearn.ensemble import RandomForestRegressor  # import model, 
from sklearn.linear_model import LinearRegression  # import model, algorithm 1
import numpy as np  # import numpy for stats
from sklearn.model_selection import train_test_split # split the training data

# train is the training data, A, B
# y is the target variable for the train data
# test is the test data, C

# split train data in 2 parts, training and validation.
training,valid, ytraining, yvalid = train_test_split(train,y,test_size=0.5) # A, B
# specify models
model1 = RandomForestRegressor() # algorithm 0
model2 = LinearRegression() # algorithm 1
# fit models
model1.fit(training, ytraining) # train algorithm 0 on A
model2.fit(training, ytraining) # train algorithm 1 on A
# make predicitons for validation
preds1 = model1.fit(valid) #  train algorithm 0 on A and make predictions for B and save to B1
preds2 = model2.fit(valid) #  train algorithm 1 on B and make predictions for B and save to B1
# make predictions for test data
test_preds1 = model1.predict(test) # train algorithm 0 on A and make predictions for C and save to C1
test_preds2 = model2.predict(test) # train algorithm 1 on A and make predictions for C and save to C1
# Form a new dataset for valid and test via stacking the predictions
stacked_predictions = np.column_stack((preds1, preds2)) # B1
stacked_test_predictions = np.column_stack((test_preds1, test_preds2)) # C1
# specify meta model
meta_model = LinearRegression() # algorithm 3 
# fit meta model on stacked predictions
meta_model.fit(stacked_predictions, yvalid) # Train algorithm 3 on B1 
# make predictions on the stacked predictions of the test data
final_predictions = meta_model.predict(stacked_test_predictions) # and make predictions for C1
```

## Things to be mindful of

- With time sensitive data - respect time

   when you create your train and validation data, you need to make certain that your train is in the past and your validation is in the future, and ideally your test data is also in the future. So you need to respect this time element in order to make certain your model generalizes well.

- Diversity as important as performance

  Therefore, what you really need to **focus** is, *am I making a model that brings some information*, even though it is generally weak? And this is true, there have been many situations where I've made, I've had some quite weak models in my ensemble, I mean, compared to the top performance. And nevertheless, they were actually adding lots of **value** in stacking. They were bringing in **new information** that the meta model could leverage.

- Diversity may come from:

  - Different algorithms
  
    Certain algorithms capitalize on **different relationships** within the data. For example, **a linear model will focus on a linear relationship**, **a non-linear model can capture better a non-linear relationships**.

  - Different input features
  
    The other thing is you can even run the same model, but you try to run it on **different transformation** of input data, either **less features or completely different transformation**. For example, in one data set you may treat categorical features as **One-hot encoding**. In another, you may just use **label** in coding, and the result will probably produce a model that is very different.

- Performance plateauing after N models

  This is a affected by:

  - How many features you have in your data?

  - How much diversity you managed to introduce into your models?

  - How many rows of data you have?

- Meta models is normally modest.

  Therefore, it is quite often that the meta model is generally **simpler**. So if I was to express this in a random forest context, it will have lower depth than what was the best one you found in your base models.

## StackNet

### StackNet as a neural network

- In a neural network, every node is a **simple linear model** (like linear regression) with some non linear transformation.

- Instead of a linear model we could use **any model**.

### How to train

- We cannot use **BP** (not all models are differentiable)

- We use **stacking** to link each model/node with target

- To extend to many levels, we can use a **Kfold** paradigm

- No epochs - different connections instead.

## Ensembling Tips and Tricks

### 1st level tips 

- Diversity based on algorithms:

  - 2-3 gradient boosted tress (lightgb,xgboost,H2O,catboost)

    Make two or three gradient boosted trees. I try to make one with bigger depth, one with middle depth, and one with low depth, and then I try to tune the parameters around it in order to make them have as similar performance as possible. Again, hoping that the end result will be some models which are fairly diverse to each other.

  - 2-3 Neural nets(keras,pytorch)

    I try to make one which is fairly deep that take three hidden layers, and one which is middle, like two hidden layers, and one that has only one hidden layer. Again, I try to diversify, I try to make them slightly different, in order to try and get new information.

  - 1-2 ExtraTrees/Random Forest (sklearn)
  - 1-2 linear models as in logistic/ridge regression, linear svm (sklearn)
  - 1-2 knn models (sklearn)
    
    Have good performance by compared to an xgboost

  - 1 Factorization machine (libfm)
  - 1 svm with nonlinear kernel if size/memory allows (sklearn)
  
    If data is not so big, useful support vector machines with non-linear kernel, like RBF

- Diversity based on input data:

  - Categorical features: One hot, label encoding, target encoding, likelihood encoding, frequency or counts.

  - Numerical features: 
  
    - outliers, binning, derivatives, percentiles, scaling

    - column one multiple column two, column one plus column tw

    - groupby statements given all categories of a categorical feature, compute the average of another variable

    - try and supervise techniques like k-means, or SVM, or PCA, or numerical features.

### Subsequent level tips

- Simpler (or shallower) Algorithms:
  - gradient boosted trees with small depth (like 2 or 3)
  - Linear models with high regularization
  - Extra Trees
  - Shallow networks (as in 1 hidden layer)
  - knn with BrayCurtis Distance
  - Brute forcing a search for best linear weights based on cv

- Feature engineering:

  - pairwrise differences between meta features
  - row-wise statistics like averages or stds
  - Standard feature selection techniques

- For every 7.5 models in previous levle we add 1 in meta

- Be mindful of target leakage

  if you see in your cross-validation that you have a next improvement, that in your test data you don't see it, then you need to go back and try to reduce the number of K-Folds. 

### Software for Stacking

- [StackNet](https://github.com/kaz-Anova/StackNet)
- [Stacked ensembles from H2O](https://www.h2o.ai/)
- [Xcessiv](https://github.com/reiinakano/xcessiv)

### Tips about StackNet

- It supports many prominent tools (xgboost, lightgbm, H2O, keras...)

- Can run classifiers in regression and vice versa.

- It has several top 10s in competitions.

- In [StackNet parameters sections](https://github.com/kaz-Anova/StackNet/blob/master/parameters/PARAMETERS.MD), tell you which parameters are the most important.

  ![3.png](3.png)

## Validation schemes for 2-nd level models



## Additional materials and links

---

- [Far0n's framework for Kaggle competitions "kaggletils"](https://github.com/Far0n/kaggletils/tree/master/kaggletils)

- [28 Jupyter Notebook tips, tricks and shortcuts](https://www.dataquest.io/blog/jupyter-notebook-tips-tricks-shortcuts/)

---

- [Kaggle ensembling guide at MLWave.com (overview of approaches)](https://mlwave.com/kaggle-ensembling-guide/)

- [StackNet — a computational, scalable and analytical meta modelling framework (by KazAnova)](https://github.com/kaz-Anova/StackNet)

- [Heamy — a set of useful tools for competitive data science (including ensembling)](https://github.com/rushter/heamy)